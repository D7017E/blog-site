---
layout: post
title:  "Week 42: Sprint 1"
date:   2022-10-18 13:00:00 +0200
published: true
---


## Block programming
### Block server
We have in, Pepper connection part, implemented head gestures, eye gestures, text-to-speech, and also translation gestures. The implementation follow the UML diagram that we did during the last sprint. 

The methods for moving is pretty general where you can define how fast you want to move pepper in every direction, such as x, y and rotation. 

The head gestures is too pretty general but you can only define one direction at a time. You can however call the function twice to get the head moving up and to the right at the same time. But the function itself can only do one thing at a time. There is also functions for shake and nod head.

The text-to-speech is simple, only send the text to Pepper so that it later can say the input text.

The expressions with the eyes is quite done, we haven't implemented any functionality for ear and shoulder expressions yet. Those expressions that exists are 
- rotate eyes, which makes a color rotate around the eyes.
- fade eyes, which fades the eyes color to another color
- angry eyes, which makes part of the eyes red and the upper part black
- sad eyes, which makes the eyes blue
- blink eyes, which makes the eyes blink
- squint eyes, which makes the eyes squint for a duration of time
- random eyes, which makes the eyes go random color for a duration of time
- wink eye, which makes one eye wink

What we have left to do is arm and hand gestures, we also need to implement a way to retrieve data from the blockly side.

### Blockly

## Web interaction

## Rock-paper-scissors


## Image recognition
